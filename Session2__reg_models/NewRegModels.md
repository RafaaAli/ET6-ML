# ðŸŒ± Discover New Regression Models
---

## ðŸ“ˆ Beyond Simple Linear Models

We looked at fitting a **straight line** to data points. However, regression can model **many kinds of relationships**, including those:

- With **multiple factors**
- Where one factor's influence depends on another

---

## ðŸ”¬ Experimenting with Models

Regression models are widely used because they are:

- Effective with **small datasets**
- **Robust** and resistant to noise
- **Interpretable**
- Available in a variety of forms

---

### ðŸ”¹ Linear Regression

- The **simplest form** of regression  
- Can handle **any number of features**  
- Comes in variants based on:
  - The **number of features**
  - The **shape** of the curve that fits the data

### âž• Linear Algorithms: Beyond Basic Linear Regression

So far, we've used **Linear Regression**, specifically the **Ordinary Least Squares (OLS)** method.  
However, linear models come in several **variants** designed to improve performance and handle more complex scenarios â€” especially when dealing with **high-dimensional data**.

One such variant is **Lasso Regression**.


### ðŸ§  What is Lasso?

**Lasso** stands for **Least Absolute Shrinkage and Selection Operator**. It's a linear regression algorithm that includes a **regularization** term â€” specifically an **L1 penalty** â€” in its cost function.

This regularization helps:

- Prevent **overfitting**
- Automatically **select important features** by shrinking some coefficients to **exactly zero**



### ðŸ§® How Lasso Works

The Lasso cost function is:

```math
Loss = MSE + Î» * Î£|wáµ¢|
```
Where:

- MSE is the Mean Squared Error

- Î» (lambda) is a regularization hyperparameter

- Î£|wáµ¢| is the sum of the absolute values of the model weights

This L1 penalty forces the model to reduce less useful weights to zero, effectively performing feature selection during training.

---

### ðŸŒ³ Decision Trees

- Use a **step-by-step** rule-based approach to predict a variable  
- Example: In our **bicycle rental** scenario:
  - The tree might first split data based on season (Spring/Summer vs. Autumn/Winter)
  - Then, make predictions based on the day of the week
  - E.g., Spring/Summer-Monday â†’ 100 rentals/day  
    Autumn/Winter-Monday â†’ 20 rentals/day

---

### ðŸ§  Ensemble Algorithms

- Combine **multiple decision trees** for stronger predictions  
- Can handle **complex and noisy datasets**  
- Popular example: **Random Forest**

> ðŸ§ª Ensemble models are widely used in real-world machine learning tasks for their **robustness** and **accuracy**.

---

## ðŸ§ª Try It Yourself

Data scientists often **experiment with multiple models** to see which performs best on a given problem.

In the following exercise, youâ€™ll try out a few different models and compare their performance on the **same dataset**.

---

### ðŸš€ Open in Google Colab

Click below to launch the notebook and experiment with model training and comparison:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-org/your-repo/blob/main/week2_regression/model_comparison.ipynb)

---
